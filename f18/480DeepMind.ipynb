{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 480DeepMind.ipynb\n",
    "\n",
    "COSC 480 - Deep Learning\n",
    "\n",
    "Fall 2018\n",
    "\n",
    "Very minor adaptation from: https://github.com/keras-rl/keras-rl/examples/dqn_atari.py\n",
    "\n",
    "Based off of: Minh et. al, \"Human-level control through deep reinforcement learning\" (2015)\n",
    "\n",
    "Notebook for DeepMind/Game Playing lecture.\n",
    "\n",
    "Installation notes:\n",
    "You will need the gym package from OpenAI as well as a handful of others that you may not have installed yet. For Linux and OSX this should be as simple as running:\n",
    "\n",
    "pip3 install gym<br>\n",
    "pip3 install gym[atari]<br>\n",
    "pip3 install h5py<br>\n",
    "pip3 install Pillow<br>\n",
    "pip3 install keras-rl\n",
    "\n",
    "For Windows? ¯\\\\_(ツ)_/¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports needed for processing, OpenAI gym, DQN\n",
    "from __future__ import division\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "\n",
    "# arguments for our simulation\n",
    "mode = 'train'\n",
    "envname = 'BreakoutDeterministic-v4'\n",
    "weights = None #if you have a previous weight file, put it here and switch mode to test\n",
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing the input and observations from the atari gym instance - note it's an image\n",
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')  # resize and convert to grayscale\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')  # saves storage in experience memory\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        # We could perform this processing step in `process_observation`. In this case, however,\n",
    "        # we would need to store a `float32` array instead, which is 4x more memory intensive than\n",
    "        # an `uint8` array. This matters if we store 1M observations.\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_2 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 1750000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.0062\n",
      "56 episodes - episode_reward: 1.107 [0.000, 6.000] - ale.lives: 2.969\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.0062\n",
      "55 episodes - episode_reward: 1.127 [0.000, 5.000] - ale.lives: 2.910\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.0058\n",
      "56 episodes - episode_reward: 1.018 [0.000, 3.000] - ale.lives: 2.949\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.0053\n",
      "58 episodes - episode_reward: 0.931 [0.000, 4.000] - ale.lives: 2.992\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.0061\n",
      "55 episodes - episode_reward: 1.073 [0.000, 4.000] - ale.lives: 2.963\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0058\n",
      "57 episodes - episode_reward: 1.035 [0.000, 4.000] - loss: 0.002 - mean_absolute_error: 0.016 - mean_q: -0.004 - mean_eps: 0.951 - ale.lives: 2.897\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0065\n",
      "57 episodes - episode_reward: 1.140 [0.000, 5.000] - loss: 0.001 - mean_absolute_error: 0.022 - mean_q: -0.004 - mean_eps: 0.942 - ale.lives: 2.827\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0066\n",
      "55 episodes - episode_reward: 1.218 [0.000, 5.000] - loss: 0.001 - mean_absolute_error: 0.028 - mean_q: 0.002 - mean_eps: 0.933 - ale.lives: 2.969\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: 0.0075\n",
      "53 episodes - episode_reward: 1.377 [0.000, 5.000] - loss: 0.001 - mean_absolute_error: 0.030 - mean_q: 0.009 - mean_eps: 0.924 - ale.lives: 2.948\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0076\n",
      "52 episodes - episode_reward: 1.481 [0.000, 5.000] - loss: 0.000 - mean_absolute_error: 0.036 - mean_q: 0.015 - mean_eps: 0.915 - ale.lives: 2.884\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: 0.0062\n",
      "56 episodes - episode_reward: 1.107 [0.000, 6.000] - loss: 0.000 - mean_absolute_error: 0.042 - mean_q: 0.021 - mean_eps: 0.906 - ale.lives: 2.913\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: 0.0078\n",
      "52 episodes - episode_reward: 1.500 [0.000, 5.000] - loss: 0.000 - mean_absolute_error: 0.047 - mean_q: 0.029 - mean_eps: 0.897 - ale.lives: 2.930\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0053\n",
      "58 episodes - episode_reward: 0.931 [0.000, 5.000] - loss: 0.000 - mean_absolute_error: 0.051 - mean_q: 0.033 - mean_eps: 0.888 - ale.lives: 2.989\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 219s 22ms/step - reward: 0.0070\n",
      "52 episodes - episode_reward: 1.308 [0.000, 6.000] - loss: 0.000 - mean_absolute_error: 0.063 - mean_q: 0.031 - mean_eps: 0.879 - ale.lives: 2.954\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: 0.0052\n",
      "59 episodes - episode_reward: 0.915 [0.000, 3.000] - loss: 0.000 - mean_absolute_error: 0.066 - mean_q: 0.035 - mean_eps: 0.870 - ale.lives: 2.973\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 225s 22ms/step - reward: 0.0065\n",
      "56 episodes - episode_reward: 1.143 [0.000, 4.000] - loss: 0.000 - mean_absolute_error: 0.070 - mean_q: 0.045 - mean_eps: 0.861 - ale.lives: 2.887\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: 0.0066\n",
      "55 episodes - episode_reward: 1.200 [0.000, 4.000] - loss: 0.000 - mean_absolute_error: 0.069 - mean_q: 0.053 - mean_eps: 0.852 - ale.lives: 2.945\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0067\n",
      "55 episodes - episode_reward: 1.236 [0.000, 5.000] - loss: 0.000 - mean_absolute_error: 0.083 - mean_q: 0.053 - mean_eps: 0.843 - ale.lives: 2.922\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 219s 22ms/step - reward: 0.0054\n",
      "58 episodes - episode_reward: 0.931 [0.000, 4.000] - loss: 0.000 - mean_absolute_error: 0.085 - mean_q: 0.060 - mean_eps: 0.834 - ale.lives: 2.897\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0062\n",
      "56 episodes - episode_reward: 1.107 [0.000, 3.000] - loss: 0.000 - mean_absolute_error: 0.089 - mean_q: 0.062 - mean_eps: 0.825 - ale.lives: 2.994\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0078\n",
      "50 episodes - episode_reward: 1.520 [0.000, 6.000] - loss: 0.000 - mean_absolute_error: 0.092 - mean_q: 0.072 - mean_eps: 0.816 - ale.lives: 2.926\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0076\n",
      "53 episodes - episode_reward: 1.434 [0.000, 5.000] - loss: 0.000 - mean_absolute_error: 0.093 - mean_q: 0.078 - mean_eps: 0.807 - ale.lives: 2.963\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: 0.0069\n",
      "53 episodes - episode_reward: 1.321 [0.000, 5.000] - loss: 0.001 - mean_absolute_error: 0.101 - mean_q: 0.086 - mean_eps: 0.798 - ale.lives: 3.003\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: 0.0076\n",
      "53 episodes - episode_reward: 1.453 [0.000, 6.000] - loss: 0.001 - mean_absolute_error: 0.107 - mean_q: 0.097 - mean_eps: 0.789 - ale.lives: 2.940\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0088\n",
      "49 episodes - episode_reward: 1.755 [0.000, 5.000] - loss: 0.001 - mean_absolute_error: 0.110 - mean_q: 0.111 - mean_eps: 0.780 - ale.lives: 2.965\n",
      "\n",
      "Interval 26 (250000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 216s 22ms/step - reward: 0.0089\n",
      "48 episodes - episode_reward: 1.896 [0.000, 7.000] - loss: 0.001 - mean_absolute_error: 0.116 - mean_q: 0.131 - mean_eps: 0.771 - ale.lives: 2.930\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: 0.0087\n",
      "49 episodes - episode_reward: 1.755 [0.000, 5.000] - loss: 0.001 - mean_absolute_error: 0.129 - mean_q: 0.155 - mean_eps: 0.762 - ale.lives: 2.948\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: 0.0100\n",
      "46 episodes - episode_reward: 2.196 [0.000, 5.000] - loss: 0.001 - mean_absolute_error: 0.139 - mean_q: 0.177 - mean_eps: 0.753 - ale.lives: 2.829\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0092\n",
      "47 episodes - episode_reward: 1.936 [0.000, 6.000] - loss: 0.001 - mean_absolute_error: 0.158 - mean_q: 0.202 - mean_eps: 0.744 - ale.lives: 2.961\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0106\n",
      "44 episodes - episode_reward: 2.432 [0.000, 6.000] - loss: 0.001 - mean_absolute_error: 0.176 - mean_q: 0.222 - mean_eps: 0.735 - ale.lives: 3.080\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: 0.0113\n",
      "43 episodes - episode_reward: 2.605 [0.000, 8.000] - loss: 0.001 - mean_absolute_error: 0.188 - mean_q: 0.254 - mean_eps: 0.726 - ale.lives: 2.994\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: 0.0109\n",
      "42 episodes - episode_reward: 2.595 [0.000, 7.000] - loss: 0.001 - mean_absolute_error: 0.211 - mean_q: 0.286 - mean_eps: 0.717 - ale.lives: 3.026\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0100\n",
      "44 episodes - episode_reward: 2.295 [0.000, 7.000] - loss: 0.001 - mean_absolute_error: 0.233 - mean_q: 0.322 - mean_eps: 0.708 - ale.lives: 3.014\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 219s 22ms/step - reward: 0.0110\n",
      "41 episodes - episode_reward: 2.683 [0.000, 8.000] - loss: 0.001 - mean_absolute_error: 0.253 - mean_q: 0.357 - mean_eps: 0.699 - ale.lives: 3.036\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0090\n",
      "47 episodes - episode_reward: 1.894 [0.000, 8.000] - loss: 0.001 - mean_absolute_error: 0.273 - mean_q: 0.384 - mean_eps: 0.690 - ale.lives: 2.813\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0095\n",
      "45 episodes - episode_reward: 2.133 [0.000, 8.000] - loss: 0.001 - mean_absolute_error: 0.296 - mean_q: 0.419 - mean_eps: 0.681 - ale.lives: 2.918\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 225s 22ms/step - reward: 0.0103\n",
      "45 episodes - episode_reward: 2.222 [0.000, 6.000] - loss: 0.004 - mean_absolute_error: 0.332 - mean_q: 0.504 - mean_eps: 0.672 - ale.lives: 3.029\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: 0.0105\n",
      "47 episodes - episode_reward: 2.234 [0.000, 8.000] - loss: 0.004 - mean_absolute_error: 0.382 - mean_q: 0.555 - mean_eps: 0.663 - ale.lives: 2.972\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 215s 21ms/step - reward: 0.0101\n",
      "48 episodes - episode_reward: 2.167 [0.000, 8.000] - loss: 0.005 - mean_absolute_error: 0.421 - mean_q: 0.594 - mean_eps: 0.654 - ale.lives: 2.846\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: 0.0110\n",
      "43 episodes - episode_reward: 2.535 [0.000, 8.000] - loss: 0.005 - mean_absolute_error: 0.454 - mean_q: 0.636 - mean_eps: 0.645 - ale.lives: 2.908\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: 0.0117\n",
      "39 episodes - episode_reward: 2.949 [0.000, 8.000] - loss: 0.004 - mean_absolute_error: 0.490 - mean_q: 0.684 - mean_eps: 0.636 - ale.lives: 2.993\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: 0.0107\n",
      "45 episodes - episode_reward: 2.422 [0.000, 10.000] - loss: 0.002 - mean_absolute_error: 0.513 - mean_q: 0.713 - mean_eps: 0.627 - ale.lives: 2.823\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 215s 21ms/step - reward: 0.0112\n",
      "41 episodes - episode_reward: 2.732 [0.000, 8.000] - loss: 0.002 - mean_absolute_error: 0.535 - mean_q: 0.741 - mean_eps: 0.618 - ale.lives: 2.987\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: 0.0128\n",
      "35 episodes - episode_reward: 3.457 [0.000, 9.000] - loss: 0.002 - mean_absolute_error: 0.558 - mean_q: 0.770 - mean_eps: 0.609 - ale.lives: 2.931\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: 0.0112\n",
      "44 episodes - episode_reward: 2.659 [0.000, 10.000] - loss: 0.001 - mean_absolute_error: 0.572 - mean_q: 0.787 - mean_eps: 0.600 - ale.lives: 2.871\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0089\n",
      "45 episodes - episode_reward: 2.044 [0.000, 8.000] - loss: 0.001 - mean_absolute_error: 0.589 - mean_q: 0.805 - mean_eps: 0.591 - ale.lives: 2.722\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0115\n",
      "42 episodes - episode_reward: 2.643 [0.000, 10.000] - loss: 0.001 - mean_absolute_error: 0.602 - mean_q: 0.820 - mean_eps: 0.582 - ale.lives: 2.976\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0128\n",
      "40 episodes - episode_reward: 3.225 [0.000, 9.000] - loss: 0.001 - mean_absolute_error: 0.614 - mean_q: 0.838 - mean_eps: 0.573 - ale.lives: 2.788\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0133\n",
      "37 episodes - episode_reward: 3.649 [0.000, 8.000] - loss: 0.001 - mean_absolute_error: 0.629 - mean_q: 0.855 - mean_eps: 0.564 - ale.lives: 2.778\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: 0.0137\n",
      "35 episodes - episode_reward: 3.943 [1.000, 9.000] - loss: 0.001 - mean_absolute_error: 0.640 - mean_q: 0.867 - mean_eps: 0.555 - ale.lives: 2.601\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0142\n",
      "35 episodes - episode_reward: 4.000 [1.000, 13.000] - loss: 0.001 - mean_absolute_error: 0.646 - mean_q: 0.875 - mean_eps: 0.546 - ale.lives: 2.966\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0130\n",
      "36 episodes - episode_reward: 3.500 [0.000, 9.000] - loss: 0.001 - mean_absolute_error: 0.658 - mean_q: 0.890 - mean_eps: 0.537 - ale.lives: 2.876\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0138\n",
      "34 episodes - episode_reward: 4.118 [0.000, 9.000] - loss: 0.001 - mean_absolute_error: 0.663 - mean_q: 0.895 - mean_eps: 0.528 - ale.lives: 2.912\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0142\n",
      "35 episodes - episode_reward: 4.143 [0.000, 9.000] - loss: 0.001 - mean_absolute_error: 0.675 - mean_q: 0.911 - mean_eps: 0.519 - ale.lives: 3.032\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0133\n",
      "33 episodes - episode_reward: 4.061 [0.000, 10.000] - loss: 0.001 - mean_absolute_error: 0.680 - mean_q: 0.916 - mean_eps: 0.510 - ale.lives: 3.160\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0161\n",
      "30 episodes - episode_reward: 5.367 [1.000, 12.000] - loss: 0.001 - mean_absolute_error: 0.684 - mean_q: 0.923 - mean_eps: 0.501 - ale.lives: 2.823\n",
      "\n",
      "Interval 57 (560000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0156\n",
      "32 episodes - episode_reward: 4.812 [1.000, 11.000] - loss: 0.001 - mean_absolute_error: 0.692 - mean_q: 0.933 - mean_eps: 0.492 - ale.lives: 2.965\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      " 8425/10000 [========================>.....] - ETA: 34s - reward: 0.0173done, took 11753.026 seconds\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 17.000, steps: 751\n",
      "Episode 2: reward: 17.000, steps: 748\n",
      "Episode 3: reward: 17.000, steps: 748\n",
      "Episode 4: reward: 17.000, steps: 748\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-824014106ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m# Finally, evaluate our algorithm for 10 episodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mweights_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dqn_{}_weights.h5f'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_repetition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                     \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(envname)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build our model. We use the same model that was described by Mnih et al. (2015).\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "if K.image_dim_ordering() == 'tf':\n",
    "    # (width, height, channels)\n",
    "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "elif K.image_dim_ordering() == 'th':\n",
    "    # (channels, width, height)\n",
    "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering.')\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "processor = AtariProcessor()\n",
    "\n",
    "# Select a policy. We use eps-greedy action selection, which means that a random action is selected\n",
    "# with probability eps. We anneal eps from 1.0 to 0.1 over the course of 1M steps. This is done so that\n",
    "# the agent initially explores the environment (high eps) and then gradually sticks to what it knows\n",
    "# (low eps). We also set a dedicated eps value that is used during testing. Note that we set it to 0.05\n",
    "# so that the agent still performs some random actions. This ensures that the agent cannot get stuck.\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
    "                              nb_steps=1000000)\n",
    "\n",
    "# The trade-off between exploration and exploitation is difficult and an on-going research topic.\n",
    "# If you want, you can experiment with the parameters or use a different policy. Another popular one\n",
    "# is Boltzmann-style exploration:\n",
    "# policy = BoltzmannQPolicy(tau=1.)\n",
    "# Feel free to give it a try!\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000,\n",
    "               train_interval=4, delta_clip=1.)\n",
    "dqn.compile(Adam(lr=.00025), metrics=['mae'])\n",
    "\n",
    "if mode == 'train':\n",
    "    # Okay, now it's time to learn something! We capture the interrupt exception so that training\n",
    "    # can be prematurely aborted. Notice that you can the built-in Keras callbacks!\n",
    "    #adjust nb_steps to lower the amount of time that this executes\n",
    "    weights_filename = 'dqn_{}_weights.h5f'.format(envname)\n",
    "    checkpoint_weights_filename = 'dqn_' + envname + '_weights_{step}.h5f'\n",
    "    log_filename = 'dqn_{}_log.json'.format(envname)\n",
    "    callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
    "    callbacks += [FileLogger(log_filename, interval=100)]\n",
    "    dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=10000, visualize=True)\n",
    "\n",
    "    # After training is done, we save the final weights one more time.\n",
    "    dqn.save_weights(weights_filename, overwrite=True)\n",
    "\n",
    "    # Finally, evaluate our algorithm for 10 episodes.\n",
    "    dqn.test(env, nb_episodes=10, visualize=False)\n",
    "elif mode == 'test':\n",
    "    weights_filename = 'dqn_{}_weights.h5f'.format(envname)\n",
    "    if weights:\n",
    "        weights_filename = weights\n",
    "    dqn.load_weights(weights_filename)\n",
    "    dqn.test(env, nb_episodes=10, visualize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
