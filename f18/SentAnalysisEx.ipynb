{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentAnalysisEx.ipynb\n",
    "Notebook for IMDB sentiment analysis example\n",
    "Heavily influenced by https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/\n",
    "Alan Jamieson\n",
    "COSC 480 - Deep Learning\n",
    "Fall 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# top_words is how many of the most frequent words we want to consider.\n",
    "# max_words is the maximum review length we want to consider.\n",
    "top_words = 10000\n",
    "max_length = 750\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words, maxlen=max_length)\n",
    "\n",
    "# Pad dataset to ensure proper shape based on size of reviews.\n",
    "# Some reviews will be shorter, and to ensure we don't get an error later, we need to make sure everything\n",
    "# is the same length.\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add our layers.\n",
    "# Word embedding input layer.\n",
    "model.add(Embedding(top_words, 32, input_length=max_length))\n",
    "\n",
    "# 1D convolutional layer.\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "\n",
    "# Max Pooling layer.\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Flattening our vector.\n",
    "model.add(Flatten())\n",
    "\n",
    "# Rectifier activation function.\n",
    "model.add(Dense(250, activation='relu'))\n",
    "\n",
    "# Sigmoid activation function.\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile, and then fit the model to the data.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
